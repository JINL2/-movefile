#!/usr/bin/env python3
"""
Step 2: Workflow API Server (EXE ë¹Œë“œìš©) - ì„¤ì • íŒŒì¼ ì‚¬ìš© ë²„ì „
ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ì„ ê´€ë¦¬í•˜ëŠ” API ì„œë²„

ë¹Œë“œ ë°©ë²•:
pip install pyinstaller flask flask-cors
pyinstaller --onefile --name "Step2_API_Server" --hidden-import=flask --hidden-import=flask_cors step2_api_server.py
"""

import os
import sys
import json
import time
import logging
import subprocess
import configparser
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional, List
from queue import Queue
from threading import Thread, Lock
import uuid

from flask import Flask, request, jsonify
from flask_cors import CORS

# ì„¤ì • íŒŒì¼ ì½ê¸°
def load_config():
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    config = configparser.ConfigParser()
    
    # ì‹¤í–‰ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì • íŒŒì¼ ì°¾ê¸°
    if getattr(sys, 'frozen', False):
        # PyInstallerë¡œ ë¹Œë“œëœ ê²½ìš°
        base_path = Path(sys.executable).parent
        config_path = base_path / 'config_step2.ini'
    else:
        # ê°œë°œ í™˜ê²½
        base_path = Path(__file__).parent
        config_path = base_path / 'config_step2.ini'
    
    if not config_path.exists():
        print(f"ERROR: ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        print("config_step2.ini íŒŒì¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
        input("Press Enter to exit...")
        sys.exit(1)
    
    config.read(config_path, encoding='utf-8')
    return config, base_path

# ì„¤ì • ë¡œë“œ
try:
    config, BASE_PATH = load_config()
    
    # ì„œë²„ ì„¤ì •
    SERVER_HOST = config.get('server', 'host')
    SERVER_PORT = config.getint('server', 'port')
    SERVER_DEBUG = config.getboolean('server', 'debug')
    
    # ì›Œí¬í”Œë¡œìš° ì„¤ì •
    WORKFLOW_BASE_PATH = config.get('workflow', 'base_path')
    DEFAULT_WORKFLOW = config.get('workflow', 'default_workflow')
    WORKFLOW_TIMEOUT = config.getint('workflow', 'timeout_seconds')
    
    # ì›Œí¬í”Œë¡œìš° ë§¤í•‘ ì½ê¸°
    WORKFLOW_MAPPING = {}
    if config.has_section('mapping'):
        for key, value in config.items('mapping'):
            WORKFLOW_MAPPING[key] = value
    
    # í ì„¤ì •
    MAX_QUEUE_SIZE = config.getint('queue', 'max_queue_size')
    WORKER_THREADS = config.getint('queue', 'worker_threads')
    
    # ë¡œê·¸ ì„¤ì •
    LOG_LEVEL = getattr(logging, config.get('logging', 'log_level', fallback='INFO'))
    LOG_RETENTION_DAYS = config.getint('logging', 'log_retention_days')
    
except Exception as e:
    print(f"ERROR: ì„¤ì • íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    print("config_step2.ini íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    input("Press Enter to exit...")
    sys.exit(1)

# ë¡œê·¸ ì„¤ì •
LOG_DIR = 'logs'
LOG_FILE = 'step2_api_server.log'

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
def create_log_directory():
    """ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±"""
    if not os.path.exists(LOG_DIR):
        os.makedirs(LOG_DIR)

# ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬
def cleanup_old_logs():
    """ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì‚­ì œ"""
    try:
        log_path = os.path.join(LOG_DIR, LOG_FILE)
        if os.path.exists(log_path):
            file_modified_time = datetime.fromtimestamp(os.path.getmtime(log_path))
            current_time = datetime.now()
            
            if (current_time - file_modified_time).days >= LOG_RETENTION_DAYS:
                os.remove(log_path)
                print(f"Old log file deleted: {log_path}")
    except Exception as e:
        print(f"Error cleaning up logs: {e}")

# ë¡œê¹… ì„¤ì •
def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    create_log_directory()
    cleanup_old_logs()
    
    log_path = os.path.join(LOG_DIR, LOG_FILE)
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    file_handler = logging.FileHandler(log_path, encoding='utf-8')
    file_handler.setFormatter(formatter)
    
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    
    logger = logging.getLogger(__name__)
    logger.setLevel(LOG_LEVEL)
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

logger = setup_logging()

# Flask ì•± ìƒì„±
app = Flask(__name__)
CORS(app)

# ì›Œí¬í”Œë¡œìš° í ê´€ë¦¬
class WorkflowQueue:
    def __init__(self):
        self.queue = Queue(maxsize=MAX_QUEUE_SIZE)
        self.running_jobs = {}
        self.completed_jobs = {}
        self.lock = Lock()
        self.worker_threads = []
        
    def add_job(self, workflow_id: str, params: dict = None) -> str:
        """ì›Œí¬í”Œë¡œìš° ì‘ì—…ì„ íì— ì¶”ê°€"""
        job_id = str(uuid.uuid4())
        job = {
            'id': job_id,
            'workflow_id': workflow_id,
            'params': params or {},
            'status': 'queued',
            'created_at': datetime.now().isoformat(),
            'started_at': None,
            'completed_at': None,
            'result': None,
            'error': None
        }
        
        with self.lock:
            if self.queue.full():
                raise Exception("Queue is full")
            self.queue.put(job)
            self.completed_jobs[job_id] = job
            
        logger.info(f"Job {job_id} added: {workflow_id}")
        return job_id
    
    def get_job_status(self, job_id: str) -> Optional[dict]:
        """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        with self.lock:
            return self.completed_jobs.get(job_id)
    
    def get_queue_status(self) -> dict:
        """í ìƒíƒœ ì¡°íšŒ"""
        with self.lock:
            running_count = len([j for j in self.completed_jobs.values() 
                               if j['status'] == 'running'])
            completed_count = len([j for j in self.completed_jobs.values() 
                                 if j['status'] == 'completed'])
            
            return {
                'queue_size': self.queue.qsize(),
                'is_running': running_count > 0,
                'running_count': running_count,
                'total_completed': completed_count,
                'max_queue_size': MAX_QUEUE_SIZE
            }

# ì „ì—­ í ì¸ìŠ¤í„´ìŠ¤
workflow_queue = WorkflowQueue()

def get_workflow_path(workflow_id: str) -> Path:
    """ì›Œí¬í”Œë¡œìš° ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° BASE_PATH ê¸°ì¤€ìœ¼ë¡œ í•´ì„
    if WORKFLOW_BASE_PATH.startswith('./') or WORKFLOW_BASE_PATH.startswith('.\\'):
        workflow_base = BASE_PATH / WORKFLOW_BASE_PATH[2:]
    elif WORKFLOW_BASE_PATH.startswith('/') or (len(WORKFLOW_BASE_PATH) > 1 and WORKFLOW_BASE_PATH[1] == ':'):
        # ì ˆëŒ€ ê²½ë¡œ
        workflow_base = Path(WORKFLOW_BASE_PATH)
    else:
        # ê¸°ë³¸ ìƒëŒ€ ê²½ë¡œ
        workflow_base = BASE_PATH / WORKFLOW_BASE_PATH
    
    workflow_path = workflow_base / workflow_id
    
    # ë””ë²„ê·¸ ì •ë³´
    logger.debug(f"BASE_PATH: {BASE_PATH}")
    logger.debug(f"WORKFLOW_BASE_PATH: {WORKFLOW_BASE_PATH}")
    logger.debug(f"workflow_base: {workflow_base}")
    logger.debug(f"workflow_path: {workflow_path}")
    logger.debug(f"workflow_path exists: {workflow_path.exists()}")
    
    return workflow_path

def execute_workflow(job: dict):
    """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
    try:
        with workflow_queue.lock:
            job['status'] = 'running'
            job['started_at'] = datetime.now().isoformat()
        
        workflow_id = job['workflow_id']
        params = job['params']
        
        logger.info(f"Executing workflow: {workflow_id}")
        
        # ì›Œí¬í”Œë¡œìš° ê²½ë¡œ í™•ì¸
        workflow_path = get_workflow_path(workflow_id)
        
        if not workflow_path.exists():
            logger.error(f"Workflow not found at: {workflow_path}")
            raise Exception(f"Workflow not found: {workflow_id} at {workflow_path}")
        
        # íŒŒë¼ë¯¸í„° ì €ì¥
        data_file = workflow_path / "_data.json"
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(params, f, ensure_ascii=False, indent=2)
        
        # main.py ì‹¤í–‰
        main_py = workflow_path / "main.py"
        if main_py.exists():
            process = subprocess.run(
                [sys.executable, str(main_py)],
                capture_output=True,
                text=True,
                cwd=str(workflow_path),
                timeout=WORKFLOW_TIMEOUT
            )
            
            if process.returncode == 0:
                try:
                    result = json.loads(process.stdout)
                except:
                    result = {
                        'status': 'completed',
                        'output': process.stdout
                    }
                
                with workflow_queue.lock:
                    job['status'] = 'completed'
                    job['result'] = result
            else:
                raise Exception(f"Workflow failed: {process.stderr}")
        else:
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì €ì¥ ì›Œí¬í”Œë¡œìš° (í…ŒìŠ¤íŠ¸ìš©)
            output_dir = workflow_path / "output"
            output_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = output_dir / f"result_{timestamp}.txt"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(f"Workflow: {workflow_id}\n")
                f.write(f"Executed at: {datetime.now()}\n")
                f.write(f"Parameters:\n{json.dumps(params, ensure_ascii=False, indent=2)}\n")
            
            with workflow_queue.lock:
                job['status'] = 'completed'
                job['result'] = {
                    'status': 'completed',
                    'output_file': str(output_file)
                }
        
        logger.info(f"Job {job['id']} completed successfully")
        
    except subprocess.TimeoutExpired:
        logger.error(f"Job {job['id']} timeout")
        with workflow_queue.lock:
            job['status'] = 'failed'
            job['error'] = 'Workflow execution timeout'
    except Exception as e:
        logger.error(f"Job {job['id']} failed: {str(e)}")
        with workflow_queue.lock:
            job['status'] = 'failed'
            job['error'] = str(e)
    finally:
        with workflow_queue.lock:
            job['completed_at'] = datetime.now().isoformat()

def worker_thread():
    """ì›Œì»¤ ìŠ¤ë ˆë“œ - íì—ì„œ ì‘ì—…ì„ ê°€ì ¸ì™€ ì‹¤í–‰"""
    logger.info("Worker thread started")
    
    last_cleanup = datetime.now()
    
    while True:
        try:
            job = workflow_queue.queue.get(timeout=1)
            execute_workflow(job)
            
            # í•˜ë£¨ì— í•œ ë²ˆ ë¡œê·¸ ì •ë¦¬
            current_time = datetime.now()
            if (current_time - last_cleanup).days >= 1:
                cleanup_old_logs()
                last_cleanup = current_time
                logger.info("ğŸ§¹ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
        except:
            # íê°€ ë¹„ì–´ìˆìœ¼ë©´ ê³„ì† ëŒ€ê¸°
            time.sleep(0.1)

# API ì—”ë“œí¬ì¸íŠ¸
@app.route('/')
def index():
    """API ì •ë³´"""
    return jsonify({
        'name': 'Workflow Automation API',
        'version': '2.0',
        'status': 'running',
        'config_file': 'config_step2.ini',
        'endpoints': {
            'GET /status': 'ì‹œìŠ¤í…œ ìƒíƒœ',
            'GET /workflows': 'ì›Œí¬í”Œë¡œìš° ëª©ë¡',
            'POST /workflows/{id}/run': 'ì›Œí¬í”Œë¡œìš° ì‹¤í–‰',
            'GET /jobs/{id}': 'ì‘ì—… ìƒíƒœ ì¡°íšŒ'
        }
    })

@app.route('/status', methods=['GET'])
def get_status():
    """ì‹œìŠ¤í…œ ìƒíƒœ"""
    status = workflow_queue.get_queue_status()
    
    # ìµœê·¼ ì‘ì—… 10ê°œ
    recent_jobs = []
    with workflow_queue.lock:
        sorted_jobs = sorted(
            workflow_queue.completed_jobs.items(),
            key=lambda x: x[1]['created_at'],
            reverse=True
        )[:10]
        
        for job_id, job in sorted_jobs:
            recent_jobs.append({
                'id': job_id,
                'workflow_id': job['workflow_id'],
                'status': job['status'],
                'created_at': job['created_at']
            })
    
    return jsonify({
        'success': True,
        'status': status,
        'recent_jobs': recent_jobs,
        'timestamp': datetime.now().isoformat()
    })

@app.route('/workflows', methods=['GET'])
def list_workflows():
    """ì›Œí¬í”Œë¡œìš° ëª©ë¡"""
    try:
        workflows = []
        workflow_base = get_workflow_path('')
        
        logger.info(f"Listing workflows from: {workflow_base}")
        
        if workflow_base.exists():
            for item in workflow_base.iterdir():
                if item.is_dir() and not item.name.startswith('_'):
                    workflows.append({
                        'id': item.name,
                        'name': item.name.replace('_', ' ').title(),
                        'available': True
                    })
        else:
            logger.warning(f"Workflow base path does not exist: {workflow_base}")
        
        return jsonify({
            'success': True,
            'workflows': workflows
        })
    except Exception as e:
        logger.error(f"Error listing workflows: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/workflows/<workflow_id>/run', methods=['POST'])
def run_workflow(workflow_id):
    """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
    try:
        params = request.json or {}
        
        # API íƒ€ì…ìœ¼ë¡œ ì›Œí¬í”Œë¡œìš° ë§¤í•‘
        api_type = params.get('api_type')
        if api_type and api_type in WORKFLOW_MAPPING:
            workflow_id = WORKFLOW_MAPPING[api_type]
            logger.info(f"Mapped {api_type} to workflow {workflow_id}")
        
        # ì›Œí¬í”Œë¡œìš° ì¡´ì¬ í™•ì¸
        workflow_path = get_workflow_path(workflow_id)
        if not workflow_path.exists():
            logger.error(f"Workflow not found: {workflow_id} at {workflow_path}")
            return jsonify({
                'success': False,
                'error': f'Workflow not found: {workflow_id}'
            }), 404
        
        # íì— ì¶”ê°€
        job_id = workflow_queue.add_job(workflow_id, params)
        
        return jsonify({
            'success': True,
            'job_id': job_id,
            'message': 'Workflow queued for execution',
            'status_url': f'/jobs/{job_id}'
        })
        
    except Exception as e:
        logger.error(f"Error running workflow: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/jobs/<job_id>', methods=['GET'])
def get_job_status(job_id):
    """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    job = workflow_queue.get_job_status(job_id)
    
    if not job:
        return jsonify({
            'success': False,
            'error': 'Job not found'
        }), 404
    
    return jsonify({
        'success': True,
        'job': job
    })

def start_workers():
    """ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘"""
    for i in range(WORKER_THREADS):
        thread = Thread(target=worker_thread, daemon=True)
        thread.start()
        workflow_queue.worker_threads.append(thread)
        logger.info(f"Started worker thread {i+1}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("=" * 60)
    logger.info("ğŸš€ Workflow API Server Starting")
    logger.info(f"ğŸ“¡ Server: http://{SERVER_HOST}:{SERVER_PORT}")
    logger.info(f"ğŸ“‚ Workflow Path: {WORKFLOW_BASE_PATH}")
    logger.info(f"ğŸ‘· Worker Threads: {WORKER_THREADS}")
    logger.info(f"ğŸ“‚ ë¡œê·¸ ìœ„ì¹˜: {os.path.join(LOG_DIR, LOG_FILE)}")
    logger.info(f"âš™ï¸  ì„¤ì • íŒŒì¼: config_step2.ini")
    logger.info("=" * 60)
    
    # ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
    start_workers()
    
    # Flask ì„œë²„ ì‹¤í–‰
    try:
        app.run(
            host=SERVER_HOST,
            port=SERVER_PORT,
            debug=SERVER_DEBUG,
            use_reloader=False  # PyInstallerì™€ ì¶©ëŒ ë°©ì§€
        )
    except Exception as e:
        logger.error(f"Server error: {str(e)}")
        input("Press Enter to exit...")
        sys.exit(1)

if __name__ == "__main__":
    main()
