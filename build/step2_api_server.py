#!/usr/bin/env python3
"""
Step 2: Workflow API Server (EXE ë¹Œë“œìš©)
ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ì„ ê´€ë¦¬í•˜ëŠ” API ì„œë²„

ë¹Œë“œ ë°©ë²•:
pip install pyinstaller flask flask-cors
pyinstaller --onefile --name "Step2_API_Server" --hidden-import=flask --hidden-import=flask_cors step2_api_server.py
"""

import os
import sys
import json
import time
import logging
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional, List
from queue import Queue
from threading import Thread, Lock
import uuid

from flask import Flask, request, jsonify
from flask_cors import CORS

# ==================== ì„¤ì • ë³€ìˆ˜ (ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ì„¸ìš”) ====================

# ì„œë²„ ì„¤ì •
SERVER_HOST = '0.0.0.0'  # ëª¨ë“  IPì—ì„œ ì ‘ì† ê°€ëŠ¥
SERVER_PORT = 5001
SERVER_DEBUG = False  # í”„ë¡œë•ì…˜ì—ì„œëŠ” False

# ì›Œí¬í”Œë¡œìš° ì„¤ì •
WORKFLOW_BASE_PATH = '../step3_workflows'  # ì›Œí¬í”Œë¡œìš° í´ë” ê²½ë¡œ
DEFAULT_WORKFLOW = 'create_contents'  # ê¸°ë³¸ ì›Œí¬í”Œë¡œìš°
WORKFLOW_TIMEOUT = 300  # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì œí•œ ì‹œê°„ (ì´ˆ)

# ì›Œí¬í”Œë¡œìš° ë§¤í•‘
WORKFLOW_MAPPING = {
    'create_contents_on_user_idea': 'create_contents',
    # ì¶”ê°€ ë§¤í•‘ì€ ì—¬ê¸°ì—
}

# ë¡œê·¸ ì„¤ì •
LOG_DIR = 'logs'  # ë¡œê·¸ ë””ë ‰í† ë¦¬
LOG_FILE = 'step2_api_server.log'
LOG_LEVEL = logging.INFO
LOG_RETENTION_DAYS = 1  # ë¡œê·¸ ë³´ê´€ ì¼ìˆ˜

# í ì„¤ì •
MAX_QUEUE_SIZE = 100  # ìµœëŒ€ í í¬ê¸°
WORKER_THREADS = 2  # ì›Œì»¤ ìŠ¤ë ˆë“œ ìˆ˜

# ========================================================================

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
def create_log_directory():
    """ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±"""
    if not os.path.exists(LOG_DIR):
        os.makedirs(LOG_DIR)

# ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬
def cleanup_old_logs():
    """ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì‚­ì œ"""
    try:
        log_path = os.path.join(LOG_DIR, LOG_FILE)
        if os.path.exists(log_path):
            # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸
            file_modified_time = datetime.fromtimestamp(os.path.getmtime(log_path))
            current_time = datetime.now()
            
            # í•˜ë£¨ ì´ìƒ ì§€ë‚œ ë¡œê·¸ íŒŒì¼ ì‚­ì œ
            if (current_time - file_modified_time).days >= LOG_RETENTION_DAYS:
                os.remove(log_path)
                print(f"Old log file deleted: {log_path}")
    except Exception as e:
        print(f"Error cleaning up logs: {e}")

# ë¡œê¹… ì„¤ì •
def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    create_log_directory()
    cleanup_old_logs()
    
    log_path = os.path.join(LOG_DIR, LOG_FILE)
    
    # ë¡œê·¸ í¬ë§·í„°
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler(log_path, encoding='utf-8')
    file_handler.setFormatter(formatter)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    
    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger(__name__)
    logger.setLevel(LOG_LEVEL)
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

logger = setup_logging()

# Flask ì•± ìƒì„±
app = Flask(__name__)
CORS(app)

# ì›Œí¬í”Œë¡œìš° í ê´€ë¦¬
class WorkflowQueue:
    def __init__(self):
        self.queue = Queue(maxsize=MAX_QUEUE_SIZE)
        self.running_jobs = {}
        self.completed_jobs = {}
        self.lock = Lock()
        self.worker_threads = []
        
    def add_job(self, workflow_id: str, params: dict = None) -> str:
        """ì›Œí¬í”Œë¡œìš° ì‘ì—…ì„ íì— ì¶”ê°€"""
        job_id = str(uuid.uuid4())
        job = {
            'id': job_id,
            'workflow_id': workflow_id,
            'params': params or {},
            'status': 'queued',
            'created_at': datetime.now().isoformat(),
            'started_at': None,
            'completed_at': None,
            'result': None,
            'error': None
        }
        
        with self.lock:
            if self.queue.full():
                raise Exception("Queue is full")
            self.queue.put(job)
            self.completed_jobs[job_id] = job
            
        logger.info(f"Job {job_id} added: {workflow_id}")
        return job_id
    
    def get_job_status(self, job_id: str) -> Optional[dict]:
        """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        with self.lock:
            return self.completed_jobs.get(job_id)
    
    def get_queue_status(self) -> dict:
        """í ìƒíƒœ ì¡°íšŒ"""
        with self.lock:
            running_count = len([j for j in self.completed_jobs.values() 
                               if j['status'] == 'running'])
            completed_count = len([j for j in self.completed_jobs.values() 
                                 if j['status'] == 'completed'])
            
            return {
                'queue_size': self.queue.qsize(),
                'is_running': running_count > 0,
                'running_count': running_count,
                'total_completed': completed_count,
                'max_queue_size': MAX_QUEUE_SIZE
            }

# ì „ì—­ í ì¸ìŠ¤í„´ìŠ¤
workflow_queue = WorkflowQueue()

def get_workflow_path(workflow_id: str) -> Path:
    """ì›Œí¬í”Œë¡œìš° ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    # ì‹¤í–‰ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
    if getattr(sys, 'frozen', False):
        # PyInstallerë¡œ ë¹Œë“œëœ ê²½ìš°
        base_path = Path(sys.executable).parent
    else:
        # ê°œë°œ í™˜ê²½
        base_path = Path(__file__).parent
    
    workflow_path = base_path / WORKFLOW_BASE_PATH / workflow_id
    return workflow_path

def execute_workflow(job: dict):
    """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
    try:
        with workflow_queue.lock:
            job['status'] = 'running'
            job['started_at'] = datetime.now().isoformat()
        
        workflow_id = job['workflow_id']
        params = job['params']
        
        logger.info(f"Executing workflow: {workflow_id}")
        
        # ì›Œí¬í”Œë¡œìš° ê²½ë¡œ í™•ì¸
        workflow_path = get_workflow_path(workflow_id)
        
        if not workflow_path.exists():
            raise Exception(f"Workflow not found: {workflow_id}")
        
        # íŒŒë¼ë¯¸í„° ì €ì¥
        data_file = workflow_path / "_data.json"
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(params, f, ensure_ascii=False, indent=2)
        
        # main.py ì‹¤í–‰
        main_py = workflow_path / "main.py"
        if main_py.exists():
            process = subprocess.run(
                [sys.executable, str(main_py)],
                capture_output=True,
                text=True,
                cwd=str(workflow_path),
                timeout=WORKFLOW_TIMEOUT
            )
            
            if process.returncode == 0:
                try:
                    result = json.loads(process.stdout)
                except:
                    result = {
                        'status': 'completed',
                        'output': process.stdout
                    }
                
                with workflow_queue.lock:
                    job['status'] = 'completed'
                    job['result'] = result
            else:
                raise Exception(f"Workflow failed: {process.stderr}")
        else:
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì €ì¥ ì›Œí¬í”Œë¡œìš° (í…ŒìŠ¤íŠ¸ìš©)
            output_dir = workflow_path / "output"
            output_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = output_dir / f"result_{timestamp}.txt"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(f"Workflow: {workflow_id}\n")
                f.write(f"Executed at: {datetime.now()}\n")
                f.write(f"Parameters:\n{json.dumps(params, ensure_ascii=False, indent=2)}\n")
            
            with workflow_queue.lock:
                job['status'] = 'completed'
                job['result'] = {
                    'status': 'completed',
                    'output_file': str(output_file)
                }
        
        logger.info(f"Job {job['id']} completed successfully")
        
    except subprocess.TimeoutExpired:
        logger.error(f"Job {job['id']} timeout")
        with workflow_queue.lock:
            job['status'] = 'failed'
            job['error'] = 'Workflow execution timeout'
    except Exception as e:
        logger.error(f"Job {job['id']} failed: {str(e)}")
        with workflow_queue.lock:
            job['status'] = 'failed'
            job['error'] = str(e)
    finally:
        with workflow_queue.lock:
            job['completed_at'] = datetime.now().isoformat()

def worker_thread():
    """ì›Œì»¤ ìŠ¤ë ˆë“œ - íì—ì„œ ì‘ì—…ì„ ê°€ì ¸ì™€ ì‹¤í–‰"""
    logger.info("Worker thread started")
    
    last_cleanup = datetime.now()
    
    while True:
        try:
            job = workflow_queue.queue.get(timeout=1)
            execute_workflow(job)
            
            # í•˜ë£¨ì— í•œ ë²ˆ ë¡œê·¸ ì •ë¦¬
            current_time = datetime.now()
            if (current_time - last_cleanup).days >= 1:
                cleanup_old_logs()
                last_cleanup = current_time
                logger.info("ğŸ§¹ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
        except:
            # íê°€ ë¹„ì–´ìˆìœ¼ë©´ ê³„ì† ëŒ€ê¸°
            time.sleep(0.1)

# API ì—”ë“œí¬ì¸íŠ¸
@app.route('/')
def index():
    """API ì •ë³´"""
    return jsonify({
        'name': 'Workflow Automation API',
        'version': '2.0',
        'status': 'running',
        'endpoints': {
            'GET /status': 'ì‹œìŠ¤í…œ ìƒíƒœ',
            'GET /workflows': 'ì›Œí¬í”Œë¡œìš° ëª©ë¡',
            'POST /workflows/{id}/run': 'ì›Œí¬í”Œë¡œìš° ì‹¤í–‰',
            'GET /jobs/{id}': 'ì‘ì—… ìƒíƒœ ì¡°íšŒ'
        }
    })

@app.route('/status', methods=['GET'])
def get_status():
    """ì‹œìŠ¤í…œ ìƒíƒœ"""
    status = workflow_queue.get_queue_status()
    
    # ìµœê·¼ ì‘ì—… 10ê°œ
    recent_jobs = []
    with workflow_queue.lock:
        sorted_jobs = sorted(
            workflow_queue.completed_jobs.items(),
            key=lambda x: x[1]['created_at'],
            reverse=True
        )[:10]
        
        for job_id, job in sorted_jobs:
            recent_jobs.append({
                'id': job_id,
                'workflow_id': job['workflow_id'],
                'status': job['status'],
                'created_at': job['created_at']
            })
    
    return jsonify({
        'success': True,
        'status': status,
        'recent_jobs': recent_jobs,
        'timestamp': datetime.now().isoformat()
    })

@app.route('/workflows', methods=['GET'])
def list_workflows():
    """ì›Œí¬í”Œë¡œìš° ëª©ë¡"""
    try:
        workflows = []
        workflow_base = get_workflow_path('')
        
        if workflow_base.exists():
            for item in workflow_base.iterdir():
                if item.is_dir() and not item.name.startswith('_'):
                    workflows.append({
                        'id': item.name,
                        'name': item.name.replace('_', ' ').title(),
                        'available': True
                    })
        
        return jsonify({
            'success': True,
            'workflows': workflows
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/workflows/<workflow_id>/run', methods=['POST'])
def run_workflow(workflow_id):
    """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
    try:
        params = request.json or {}
        
        # API íƒ€ì…ìœ¼ë¡œ ì›Œí¬í”Œë¡œìš° ë§¤í•‘
        api_type = params.get('api_type')
        if api_type and api_type in WORKFLOW_MAPPING:
            workflow_id = WORKFLOW_MAPPING[api_type]
            logger.info(f"Mapped {api_type} to workflow {workflow_id}")
        
        # ì›Œí¬í”Œë¡œìš° ì¡´ì¬ í™•ì¸
        workflow_path = get_workflow_path(workflow_id)
        if not workflow_path.exists():
            return jsonify({
                'success': False,
                'error': f'Workflow not found: {workflow_id}'
            }), 404
        
        # íì— ì¶”ê°€
        job_id = workflow_queue.add_job(workflow_id, params)
        
        return jsonify({
            'success': True,
            'job_id': job_id,
            'message': 'Workflow queued for execution',
            'status_url': f'/jobs/{job_id}'
        })
        
    except Exception as e:
        logger.error(f"Error running workflow: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/jobs/<job_id>', methods=['GET'])
def get_job_status(job_id):
    """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    job = workflow_queue.get_job_status(job_id)
    
    if not job:
        return jsonify({
            'success': False,
            'error': 'Job not found'
        }), 404
    
    return jsonify({
        'success': True,
        'job': job
    })

def start_workers():
    """ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘"""
    for i in range(WORKER_THREADS):
        thread = Thread(target=worker_thread, daemon=True)
        thread.start()
        workflow_queue.worker_threads.append(thread)
        logger.info(f"Started worker thread {i+1}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("=" * 60)
    logger.info("ğŸš€ Workflow API Server Starting")
    logger.info(f"ğŸ“¡ Server: http://{SERVER_HOST}:{SERVER_PORT}")
    logger.info(f"ğŸ“‚ Workflow Path: {WORKFLOW_BASE_PATH}")
    logger.info(f"ğŸ‘· Worker Threads: {WORKER_THREADS}")
    logger.info(f"ğŸ“‚ ë¡œê·¸ ìœ„ì¹˜: {os.path.join(LOG_DIR, LOG_FILE)}")
    logger.info("=" * 60)
    
    # ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
    start_workers()
    
    # Flask ì„œë²„ ì‹¤í–‰
    try:
        app.run(
            host=SERVER_HOST,
            port=SERVER_PORT,
            debug=SERVER_DEBUG,
            use_reloader=False  # PyInstallerì™€ ì¶©ëŒ ë°©ì§€
        )
    except Exception as e:
        logger.error(f"Server error: {str(e)}")
        input("Press Enter to exit...")
        sys.exit(1)

if __name__ == "__main__":
    main()
